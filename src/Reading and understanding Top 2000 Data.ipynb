{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.preprocessing\n",
    "import sklearn.metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "import top2000analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_OUTPUT = os.path.join('..', 'Output')\n",
    "FOLDER_DATA = os.path.join('..', 'Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently, I published a [blog](https://sjoerdcornelissen.com/2021/03/02/the-effect-of-passing-away-on-top-2000-ranking/.) on the effect of passing away on rakings in the Top 2000. There, I dived deep into the outcomes of the model. Additionally, I also want to show how to build such a model and do the analysis, which I will do in this blog series.\n",
    "\n",
    "I will do so in three parts:\n",
    "1. *Part 1: Reading and understanding the data*  \n",
    "  In this part, I will acquire the data and get a first feel for it.\n",
    "1. *Part 2: Building and checking a hierarchical Bayesian regresson model*  \n",
    "In this part, we will use the data and knowledge about it we acquired in the first part to actually build a hierachical, Bayesian model. Part of building a Bayesian model is also checking whether it works as expected.\n",
    "1. *Part 3: Using the model for inference and prediction*  \n",
    "  In the final part, I will show how to use the model to get the insights we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS NOTEBOOK IS STILL UNDER CONSTRUCTION AND WILL BE UPDATED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and understanding the Top 2000 data \n",
    "\n",
    "No data science without data. While it is often the least sexy part, acquiring, structuring and importantly understanding the data well will set a project up for success, while skimping this might look good in the short term, but always turns out to hit you hard much quicker than expected, when weird results keep popping up and find out whether this is a problem with the data, the code or there is actually something interesting going up turns into an almost impossible task.\n",
    "\n",
    "In this part, we will do three things:\n",
    "1. _Acquiring the data_  \n",
    "We scrape wikipedia for everything about the Top 2000\n",
    "1. _Structuring the data_  \n",
    "By putting the data into tables that are structured through a data model, we keep all the flexibility for future analyses.\n",
    "1. _Understanding the data_  \n",
    "By looking into the univariate correlations, we get a feeling for what influences what, which will help us when building a model.\n",
    "\n",
    "## Acquiring the data: scraping wikipedia\n",
    "First, we scrape all the data from Wikipedia. While NPO Radio 2, the organizer, also publishes the list in Excel form on the [Top 2000 website](https://www.nporadio2.nl/top2000), I did not use that for two reasons:\n",
    "* It uses an inconsistent format through the years, both in column names and band names.\n",
    "* For the more detailed artist information I need, I would have to use an external source anyway, so it is easier to immediately use Wikipedia.\n",
    "\n",
    "Wikipedia has complete rankings of all the previous Top 2000, directly with links to more detailed pages about the artists and songs: https://nl.wikipedia.org/wiki/Lijst_van_Radio_2-Top_2000%27s.\n",
    "\n",
    "While there are libraries that access Wikipedia in python, such as [`wikipedia`](https://pypi.org/project/wikipedia/) and [`Wikipedia-API`](https://pypi.org/project/Wikipedia-API/), I did not use those:\n",
    "* They are generally oriented towards a user-oriented way of looking through Wikipidia, with a user searching and reading the main text. I, on the other hand, am focused on extracting the Tables and Infoboxes, and there was no good support for that. \n",
    "* The link(s) that are in the cells are important, since we will want to browse those for more detailed information. This generally does not come out of the box.\n",
    "\n",
    "Therefore, I decided to write a very quick module that reads Tables and Infoboxes from Wikipedia. It can be found on my Github as [`wikipediareader`](https://github.com/SjoerdCor/Top2000/blob/main/src/wikipediareader.py).\n",
    "\n",
    "This module is used by the `readtop2000` module, which reads and then writes the information we need about the Top2000\n",
    "1. Scrapes the full list (https://nl.wikipedia.org/wiki/Lijst_van_Radio_2-Top_2000%27s)\n",
    "1. Goes to all artist pages it finds on the full list and adds the information from the Infobox on that page to the table\n",
    "1. Cleans the column names and types and validates the reading worked\n",
    "1. Writes the information to a structured data model.\n",
    "\n",
    "We store all the tables as _.parquet_-files, a commonly used format that also store data types and is memory efficient and quick to read and write."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structuring the data\n",
    "### Splitting data into a data model\n",
    "By thinking about how we should structure and save the data, we save ourselves from a lot of pain down the road. I decided to save the information according to the following data model:\n",
    "\n",
    "<img src=\"../blogfigures/Reading and understanding Top 2000 data/datamodel.PNG\" title=\"The data model consists of ranking table with a many-to-one relationship to song, which has a one to many relationship to songartist, which has a many-to-one relationship with artist\"/>\n",
    "We have the following entities:\n",
    "\n",
    "* A **ranking** for each song for each year in the Top 2000\n",
    "* A **song** which is performed by one or more artists\n",
    "* An **artist** who performs one or more songs.\n",
    "\n",
    "The most notable as aspect is that **song** and **artist** have a [many-to-many relationship](https://en.wikipedia.org/wiki/Many-to-many_(data_model)), so we add a junction table between those. The artist table contains a column for every label that is used on at least one artist, so it has a lot columns and a lot of `NaN`s.\n",
    "\n",
    "Now that we have read and structured all the data, we can finally get the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import readtop2000\n",
    "import importlib; importlib.reload(readtop2000)\n",
    "downloader = readtop2000.Top2000Downloader()\n",
    "downloader.download_and_write(FOLDER_OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recombining and selecting the data\n",
    "Now, we wish to get a table on which we can perform the analysis. For this, we combine all data, with one row for each artist for each song, so a song which is a duet occurs twice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib; importlib.reload(top2000analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = top2000analysis.AnalysisSetCreator(FOLDER_DATA)\n",
    "\n",
    "df_full = a._combine_data()\n",
    "print(f\"In its history, the Top 2000 has seen {df_full['SongID'].nunique()} unique songs and {df_full['ArtistID'].nunique()} unique artists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our analysis, we only select the rows of the years in which the artist passed away, and add the information of the year before, so we can compare the number of votes.\n",
    "\n",
    "We also calculate the percentage of the vote each song received every year based on a model from [Peter Meindertsma](https://www.petermeindertsma.nl/blog/benadering-aantal-stemmen-per-liedje-in-de-top-2000-van-2014/). We end up with the dataframe as shown below, which contains all the information we need to perform the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artist = a.create_artist_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = a.create_full_feature_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The final data contains {len(df)} rows, which means there are {len(df)} songs which were in the Top 2000 the year before the artist passed away.')\n",
    "print(f'They come from {df[\"ArtistID\"].nunique()} unique artists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df['Year'].value_counts().sort_index().plot(ylim=(0, None), xlabel='Year', ylabel='Nr of artists\\nwho passed away')\n",
    "ax.xaxis.set_major_locator(mtick.MaxNLocator(integer=True))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df['Year'].value_counts().sort_index().cumsum().plot(ylim=(0, None), xlabel='Year', ylabel='Nr of artists\\nwho passed away')\n",
    "ax.xaxis.set_major_locator(mtick.MaxNLocator(integer=True))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a number of artists passing awayt each year, with a spike in 2016.\n",
    "\n",
    "Next, let's see what happened to the ranking of each song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.plot(kind='scatter', x='RankLastYear', y='Rank', label='Song of artist who passed away')\n",
    "ax.plot([0, 2000], [0, 2000], 'k--', label='Position last year')\n",
    "ax.set_xlabel('Ranking before passing away')\n",
    "ax.set_ylabel('Ranking in year of passing away')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most dots are below the dashed line, indeed almost all songs (but notably not all) receive some sort of boost in the year the artist passed away. We also see there are huge differences in the size of the effect: with some songs ranking below 1500 entering the Top 250 a year later, while some Top 500 songs stay right where they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the analysis, the discrete nature of rankings is a problem; the difference between songs ranked 2000 and 1980 is very different from the songs ranked 21st and first. Therefore, we estimate the number of votes through a model developed by [Peter Meindertsma](https://www.petermeindertsma.nl/blog/benadering-aantal-stemmen-per-liedje-in-de-top-2000-van-2014/).\n",
    "Furthermore, we define a _Boost_  as the ratio between the percentage of votes of the song in the year of the artist passing away and the percentage of the vote of that song in the year before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(series, dist='norm', **kwargs):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    series.plot(kind='kde', ax=axes[0], **kwargs)\n",
    "    scipy.stats.probplot(series, dist=dist, plot=axes[1])\n",
    "    plt.show()\n",
    "    display(series.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(df['BoostSong'], xlim=(0, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the average song gets a boost of 2.3x as many votes as the year before, and the median is 1.75x as many votes. Notably, the Boost distrution is not Normal. Therefore, we also inspect the logarithm of the boost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(df['LogBoost'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still not perfectly normal, but much better. In our further estimation, we will often work with the logarithm, because the regular boost is dominated by a few outliers.\n",
    "\n",
    "Next, we will also investigate the _artist boost_, defined as the the total number of votes for an artist in the year of his death divided by the total number of votes in the year before, instead of each song individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(df_artist['Boost'], xlim=(0, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see the same distribution. Notably, the average _artist_ boost is lower than the average _song_ boost. This can be explained if artists with multiple songs on average see a larger boost. We make the same plot, but now binned for easier interpretation, to show in the blog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_waterfall(data, color=None, buildup=False, **kwargs):\n",
    "    '''\n",
    "    Plot a buildup or builddown waterfall chart from data\n",
    "    This function was adapted from https://pbpython.com/waterfall-chart.html\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pd.Series to be shown as waterfall\n",
    "    color: optionally give color as a list for each bar (to highlight some bars)\n",
    "    buildup: False (default) for builddown, True for buildup\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ax: Axis object\n",
    "    data: the data, including a \"total\"-row\n",
    "    blank: the size of the blank space before each bar\n",
    "    '''\n",
    "    if color is None:\n",
    "        color = ['lightgray'] * len(data)\n",
    "\n",
    "    blank = data.cumsum().shift(1).fillna(0)\n",
    "    total = data.sum()\n",
    "    data.loc['Total'] = total\n",
    "    blank.loc['Total'] = 0\n",
    "    color = color + ['gray']\n",
    "    \n",
    "    step = blank.reset_index(drop=True).repeat(3).shift(-1)\n",
    "    step[1::3] = np.nan\n",
    "    \n",
    "    if buildup:\n",
    "        data = data[::-1]\n",
    "        blank = blank[::-1]\n",
    "        color = color[::-1]\n",
    "\n",
    "    ax = data.plot(kind='barh', stacked=True, left=blank, color=color, **kwargs)\n",
    "    return ax, data, blank\n",
    "\n",
    "data = pd.cut(df_artist['Boost'], [0, 1, 1.25, 1.5, 2.5, np.inf],\n",
    "               labels=['No boost',\n",
    "                       'Up to 25% more votes',\n",
    "                       'Up to 50% more votes',\n",
    "                       '1.5 - 2.5 x\\nas many votes',\n",
    "                       'More than 2.5\\nx as many votes']).value_counts(normalize=True).sort_index(ascending=False)\n",
    "data.index = data.index.astype(str)\n",
    "ax, _, _ = plot_waterfall(data, color=['purple', 'purple', 'purple', 'purple', 'lightgray', 'gray'])\n",
    "ax.set_ylabel('Effect size')\n",
    "ax.xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "# plt.gcf().savefig(os.path.join(FOLDER_OUTPUT,'BoostDistribution.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check who got the smallest and largest boosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_artist.query('Boost < 1')[['Name', 'Boost', 'LogPopularityNorm']].sort_values('Boost'))\n",
    "display(df_artist.nlargest(10, 'Boost')[['Name', 'Boost', 'LogPopularityNorm']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate analysis\n",
    "\n",
    "To get a first feeling for the data, formulate hypotheses on what affects the size of the boost on three levels:\n",
    "1. A __base__ effect for every artist\n",
    "1. An __artist specific__ effect, based on the characteristics of the artist and his/her death - aspects that have to do with how prominent the artist will be in the news after his/her death\n",
    "1. A __song specific__ effect, for which songs channel the newfound votes.\n",
    "\n",
    "For each step, the hypotheses are shown in the table below:\n",
    "\n",
    "<img src=\"../blogfigures/Hypothesis.png\" alt=\"Hypotheses are formulated for each level, to predict the boost of a song\" width=\"750\"/>\n",
    "\n",
    "In this section, we want to get a first look into the univariate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive in, lets check the relative importance of the levels. For this, we look into artist who have multiple songs, and compare the explained variance score for the song boost as explained by the _artist_ boost on the one hand, and the _Relative Song_ boost (the boost of the song divided by the artist boost) on the other hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multiple_songs = df.query('NrsBeforeDeath >= 2')\n",
    "print(f'There are {len(df_multiple_songs)} songs by artists with at least two songs')\n",
    "artist_variance = sklearn.metrics.explained_variance_score(df_multiple_songs['LogBoost'], np.log(df_multiple_songs['BoostArtist']))\n",
    "song_variance = sklearn.metrics.explained_variance_score(df_multiple_songs['LogBoost'], df_multiple_songs['LogRelativeBoost'])\n",
    "print(f'The explained variance of the artist is {artist_variance: .1%}')\n",
    "print(f'The explained variance of the relative song boost is {song_variance: .1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.violinplot(x='NrsBeforeDeath', y='SongRelativeBoost', data=df)\n",
    "ax.set_ylim(0,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The artist is most important, but we also see there is definetly still a lot of variance within the songs of an artist, which we should try to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_trend(df, column, logy=False):\n",
    "    ycolname = 'LogBoost' if logy else 'Boost'\n",
    "    preds = LinearRegression().fit(df[[column]], df[ycolname]).predict(df[[column]])\n",
    "    ax = df.plot(x=column, y=ycolname, kind='scatter', label='Passed away artists', c='grey')\n",
    "    ax.plot(df[column], preds, 'k', label='Trend')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artist level\n",
    "\n",
    "### Artist age\n",
    "Newsworthiness of the death of an artist depends on the circumstances. Specifically, artists dying young is something that attracts a lot of media attention. This happens a lot in our sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_dying_before_80 = df_artist['AgePassing'].lt(80).mean()\n",
    "print(f'{pct_dying_before_80:.1%} of the artists who passed away were younger than 80')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_with_trend(df_artist, 'AgePassing', logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not see a strong _univariate_ effect, but this may be because of collinearity with other effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaar\n",
    "We hypothesize the boost has grown stronger over the years, because of more prevalent news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_with_trend(df_artist, 'JaarTop2000', logy=True)\n",
    "ax.xaxis.set_major_locator(mtick.MaxNLocator(integer=True))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "ax.set_frame_on(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nationality\n",
    "We hypothesize Dutch voters care more about Dutch artists, out of chauvinism. The effect is hard to detect, since only a small number of Dutch artists passed away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{df_artist[\"IsDutch\"].sum()} Dutch artists passed away')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.catplot(x='IsDutch', y='LogBoost', data=df_artist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popularity\n",
    "When more popular artists die, they get much more news coverage: a relatively unknown artist may get a small article on page 15, while superstars may get entire TV shows devoted to hem. This in turn would result in much more prominent boosts for more popular artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_with_trend(df_artist, 'PctVotes', logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be the case, but the effect between the _logarithm_  of the boost and the percentage of the votes is clearly non-linear, and the trendline is dominated by a few superstars. Let's look into the logarith of the percentage of votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_trend(df_artist, 'LogPopularity', logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better: there is a strong, mostly linear relation between the logarith of the boost and the logarithm of the popularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recency\n",
    "We hypothesize that the more top of mind a death is, the more of a boost an artist can expect - so more recent deaths during the time of voting should get a larger boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_with_trend(df_artist.query('DaysToStemperiode > -365'), 'DaysToStemperiode', logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, this is the case; however, it is not entirely clear if this is a linear relationship. Lets bucket the observations to see that clearer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recency_buckets = (pd.cut(df_artist['DaysToStemperiode'].clip(lower=-365), 6, labels=False, retbins=False, right=False)\n",
    "                  .map({i: v for i, v in enumerate(range(-333, 0, 60))}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artist.groupby(recency_buckets)['LogBoost'].agg(['mean', 'sem', 'std', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_artist.groupby(recency_buckets)['LogBoost'].agg(['mean', 'sem', 'std', 'count']).plot(y='mean', yerr='sem')\n",
    "plt.xlim(-365, 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be little difference out to ~ 150 days, and then the boost grows much stronger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recency of last hit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, there might be an effect whether artists were recently popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_with_trend(df_artist, 'YearsSinceLastHit', logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that, probably mostly by coincidence, in recent years artists in the Top 2000 have tended to pass away at later ages, which may partly explain why we do not see a strong age effect in a univariate distribution. Furthermore, we see that in recent years more popular artists have tended to pass away, which may spuri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlations(df, figsize=(12, 12)):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax = sns.heatmap(df.corr(), cmap='RdBu_r', vmin=-0.8, vmax=0.8, annot=True, fmt='.1%', ax=ax, cbar=False)\n",
    "    return ax\n",
    "\n",
    "plot_correlations(df_artist[['AgePassing', 'JaarTop2000', 'DaysToStemperiode', 'LogPopularity', 'IsDutch']], figsize=(6, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Song"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solo song\n",
    "We hypothesize that solo songs get more of a boost than duets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['NrArtists'])['LogBoost'].agg(['mean', 'std', 'sem', 'count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only one song that has more than 2 performers (namely, _Laat me/Vivre_ by Alderliefste, Liesbeth List and Ramses Shaffy; the latter two have passed away during the Top 2000), so let's group that together, since the errors are huge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.groupby(['MultiplePerformers'])['LogBoost'].agg(['mean', 'std', 'sem', 'count']))\n",
    "(df.groupby(['MultiplePerformers'])\n",
    " ['LogBoost'].agg(['mean', 'std', 'sem', 'count'])\n",
    " .plot(y='mean', yerr='sem',\n",
    "       xlim=(-0.5, 1.5), kind='bar')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are not a lot of duets, but there is a hint they may indeed have a smaller boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popularity within artist oeuvre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [literature](https://link.springer.com/article/10.1007/s11002-014-9322-1) there is some work that shows more popular albums get larger boosts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_with_trend(df, 'PopularityWithinArtist', logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not see that effect very strongly, but again, we are dominated by a few outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_with_trend(df, 'LogSongPopularityWithinArtist', logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, there appears to be a contrary effect. This may be explained, however, by a) our assumptions of the voting distribution and/or b) collinearity with other effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recency within artist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hypothesize the more recent work may receive more of a boost, since the older work may be somewhat forgotten by now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_with_trend(df.query('NrsBeforeDeath > 2'), 'RecencyWithinArtist', logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not see that effect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Top2000H",
   "language": "python",
   "name": "top2000h"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

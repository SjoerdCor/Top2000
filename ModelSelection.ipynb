{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.preprocessing\n",
    "import sklearn.metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns\n",
    "\n",
    "import pymc3 as pm\n",
    "import arviz as az\n",
    "\n",
    "import voteestimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalysisSetCreator:\n",
    "    \n",
    "    def __init__(self, votesmodel='Meindertsma'):\n",
    "\n",
    "        votesmodels = {'Meindertsma': voteestimator.MeindertsmaVotesEstimator(),\n",
    "                      'Exponential': voteestimator.ExponentialVotesEstimator()\n",
    "                      }\n",
    "        self.votesmodel = votesmodels[votesmodel]\n",
    "    \n",
    "    def _combine_data(self, filefolder):\n",
    "        self.notering = pd.read_parquet(os.path.join(filefolder, 'notering.parquet'))\n",
    "        self.song = pd.read_parquet(os.path.join(filefolder, 'song.parquet'))\n",
    "        self.songartist = pd.read_parquet(os.path.join(filefolder, 'songartist.parquet'))\n",
    "        self.artist = (pd.read_parquet(os.path.join(filefolder, 'artist.parquet')) # TODO: This should not happen here\n",
    "                          .pipe(self._artist_features)\n",
    "                        )\n",
    "        \n",
    "        df = (self.notering.merge(self.song, left_on='SongID', right_index=True)\n",
    "                           .merge(self.songartist.reset_index())\n",
    "                           .merge(self.artist, left_on='ArtistID', right_index=True, suffixes=('Song', 'Artist'))\n",
    "             )\n",
    "        return df\n",
    "    \n",
    "    def _read_stemperiodes(self, path=os.path.join('Data', 'EindeStemperiode.xlsx')):\n",
    "        einde_stemperiode = (pd.read_excel(path, engine='openpyxl')  # openpyxl does support xlsx\n",
    "                               .dropna(subset=['EindeStemperiode'])\n",
    "                               .drop(columns=['Bron'])\n",
    "                               .sort_values('EindeStemperiode')\n",
    "                            )\n",
    "        return einde_stemperiode\n",
    "    \n",
    "\n",
    "    def _check_passed_away_during_top2000(self, df, top2000_stemperiodes):\n",
    "        first_stemperiode = top2000_stemperiodes['EindeStemperiode'].min()\n",
    "        relevant_date_of_death = first_stemperiode + pd.Timedelta('365 days')\n",
    "        df['IsOverleden'] = df['Overlijdensdatum'].ge(relevant_date_of_death)\n",
    "        return df\n",
    "    \n",
    "    def _find_next_top2000_after_death(self, df, top2000_stemperiodes):\n",
    "        not_passed_away_during_top_2000 = df[~df['IsOverleden']].copy()\n",
    "        passed_away_during_top2000 = (df.loc[df['IsOverleden']]\n",
    "                                      .sort_values('Overlijdensdatum')\n",
    "                                      .reset_index()\n",
    "                                     )\n",
    "\n",
    "        passed_away_during_top2000 = (pd.merge_asof(passed_away_during_top2000, top2000_stemperiodes,\n",
    "                                                   left_on='Overlijdensdatum', right_on='EindeStemperiode', direction='forward')\n",
    "                                     .set_index('ArtistID')\n",
    "                                     )\n",
    "        df = pd.concat([not_passed_away_during_top_2000, passed_away_during_top2000], sort=False)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def _artist_features(self, df):\n",
    "        einde_stemperiode = self._read_stemperiodes()\n",
    "        df = (df                                          \n",
    "                .pipe(self._check_passed_away_during_top2000, einde_stemperiode)\n",
    "                .pipe(self._find_next_top2000_after_death, einde_stemperiode)\n",
    "                .assign(AgePassing = lambda df: df['Overlijdensdatum'].sub(df['Geboortedatum']).dt.days / 365.25,\n",
    "                        PassingTooEarly = lambda df: df['AgePassing'].sub(80).mul(-1).clip(lower=0),\n",
    "                        IsDutch = lambda df: df['IsDutch'].astype(int),\n",
    "                        )\n",
    "             )\n",
    "        return df\n",
    "    \n",
    "    def _rank_features(self, df):\n",
    "        return df.assign(PctVotes = lambda df: df['Rank'].apply(self.votesmodel.percentage_of_votes))\n",
    "    \n",
    "    \n",
    "    def _normalize_by_years_before_death(self, df, years_to_normalize=1):        \n",
    "        mi = pd.MultiIndex.from_product([df.query('IsOverleden')['SongID'].unique(),\n",
    "                                         df.query('IsOverleden')['YearsSinceOverlijden'].unique(),],\n",
    "                                        names=['SongID', 'YearsSinceOverlijden'])\n",
    "        votes_before_death = (pd.DataFrame(index=mi)\n",
    "                              .join(self.songartist)\n",
    "                              .join(df.set_index(['SongID', 'YearsSinceOverlijden', 'ArtistID'])[['Year', 'PctVotes']])\n",
    "                              .join(self.artist[['JaarTop2000']])\n",
    "                              .join(self.song[['YearMade']])\n",
    "                              .assign(YearTop2000 = lambda df: df['JaarTop2000'].add(df.index.get_level_values('YearsSinceOverlijden')),\n",
    "                                      PctVotes = lambda df: np.where(df['YearTop2000'].gt(df['YearMade']) & df['YearTop2000'].le(df['Year'].max()),\n",
    "                                                             df['PctVotes'].fillna(self.votesmodel.lower_than_2000), np.nan)\n",
    "                                     )\n",
    "                             ['PctVotes']\n",
    "                             .unstack('YearsSinceOverlijden')\n",
    "                             .loc[:, range(-years_to_normalize, 0)]\n",
    "                             .mean(axis='columns')\n",
    "                             .rename('PctVotesBeforeDeath')\n",
    "                             .reset_index()\n",
    "                             )\n",
    "        \n",
    "        df = df.merge(votes_before_death, how='left')\n",
    "        return df\n",
    "\n",
    "    \n",
    "    def _song_features(self, df):\n",
    "        \n",
    "        df = (df.assign(NrArtists = lambda df: df.groupby(['SongID', 'Year'])['Rank'].transform('count'),\n",
    "                        YearsSinceOverlijden = lambda df: df['Year'].sub(df['JaarTop2000']),\n",
    "                       )\n",
    "                .pipe(self._normalize_by_years_before_death)\n",
    "             )\n",
    "        return df\n",
    "    \n",
    "    def _song_features_after_passing(self, df):\n",
    "        df = (df.assign(NrsBeforeDeath = lambda df: df.groupby('ArtistID')['ArtistID'].transform('count'),\n",
    "                        PopularityWithinArtist = lambda df: df.groupby('ArtistID')['PctVotesBeforeDeath'].apply(lambda v: v.div(v.mean())),\n",
    "                        LogSongPopularityWithinArtist = lambda df: np.log10(df['PopularityWithinArtist']),\n",
    "                        RecencyWithinArtist = lambda df: df.groupby('ArtistID')['YearMade'].apply(lambda v: v.sub(v.min()).div(v.max() - v.min())),\n",
    "                        YearsBeforeDeath = lambda df: df['YearMade'].sub(df['JaarTop2000']),\n",
    "                        Boost = lambda df: df['PctVotes'].div(df['PctVotesBeforeDeath']),\n",
    "                        MultiplePerformers = lambda df: df['NrArtists'].gt(1).astype(int),\n",
    "                        JarenGeleden = lambda df: df['JaarTop2000'].sub(df['JaarTop2000'].max()),\n",
    "                        )\n",
    "             )\n",
    "        return df\n",
    "    \n",
    "    def create_analysis_set(self, filefolder):\n",
    "        df = (self._combine_data(filefolder)\n",
    "                  .pipe(self._rank_features)\n",
    "                  .pipe(self._song_features)\n",
    "                  .query('YearsSinceOverlijden == 0')\n",
    "                  .query(f'PctVotesBeforeDeath > {self.votesmodel.lower_than_2000}')\n",
    "                  .pipe(self._song_features_after_passing)\n",
    "             )\n",
    "        return df\n",
    "    \n",
    "    def create_artist_set(self, filefolder):\n",
    "        df = self.create_analysis_set(filefolder)\n",
    "        df_artist = (df.groupby('ArtistID')\n",
    "                        .agg({'PctVotes': 'sum',\n",
    "                              'PctVotesBeforeDeath': 'sum',\n",
    "                               'YearMade': 'last'\n",
    "                            }\n",
    "                            )\n",
    "                        .join(self.artist[['Name', 'IsDutch', 'AgePassing', 'JaarTop2000', 'Overlijdensdatum', 'EindeStemperiode']])\n",
    "                        .assign(DaysToStemperiode = lambda df: df['Overlijdensdatum'].sub(df['EindeStemperiode']).dt.days,\n",
    "                                YearsSinceLastHit = lambda df: df['JaarTop2000'].sub(df['YearMade']),\n",
    "                                LogPopularity = lambda df: np.log10(df['PctVotesBeforeDeath']),\n",
    "                                LogPopularityNorm = lambda df: df['LogPopularity'].sub(df['LogPopularity'].median()),\n",
    "                                Boost = lambda df: df['PctVotes'].div(df['PctVotesBeforeDeath']),\n",
    "                                LogBoost = lambda df: np.log(df['Boost']),\n",
    "                                )\n",
    "                    )\n",
    "        return df_artist\n",
    "    \n",
    "    def create_full_feature_set(self, filefolder):\n",
    "        df = self.create_analysis_set(filefolder)\n",
    "        df_artist = self.create_artist_set(filefolder)#.pipe(self._artist_features)\n",
    "        full_set = (df.merge(df_artist, left_on='ArtistID', right_index=True, suffixes=('Song', 'Artist'))\n",
    "                      .assign(\n",
    "                              SongRelativeBoost = lambda df: df['BoostSong'].div(df['BoostArtist']),\n",
    "                              LogRelativeBoost = lambda df: np.log2(df['SongRelativeBoost']),\n",
    "                              LogBoost = lambda df: np.log(df['BoostSong']),\n",
    "                             )\n",
    "           )\n",
    "        return full_set\n",
    "\n",
    "a = AnalysisSetCreator()\n",
    "df_song = a.create_analysis_set('Data')\n",
    "df_artist = a.create_artist_set('Data')\n",
    "df = a.create_full_feature_set('Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full out modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['JarenGeleden',\n",
    " 'YearMadeSong',\n",
    " 'AgePassingArtist',\n",
    " 'NrArtists',\n",
    " 'PctVotesBeforeDeathSong',\n",
    " 'NrsBeforeDeath',\n",
    " 'PopularityWithinArtist',\n",
    "'LogSongPopularityWithinArtist',\n",
    "'DaysToStemperiode',\n",
    " 'LogPopularity',\n",
    " 'IsDutchArtist'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df[features].assign(IsDutchArtist = lambda df: df['IsDutchArtist'].astype(int))\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, df['LogBoost'], random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['JarenGeleden'].value_counts(normalize=True).sort_index().cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(X, y, jaren_geleden_split=-4):\n",
    "    train_set = X['JarenGeleden'].le(jaren_geleden_split)\n",
    "    X_train = X[train_set].copy()\n",
    "    y_train = y[train_set].copy()\n",
    "    X_test = X[~train_set].copy()\n",
    "    y_test = y[~train_set].copy()\n",
    "    return X_train, X_test, y_train, y_test\n",
    "X_train, X_test, y_train, y_test = split_train_test(X, df['LogBoost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test, y_train, y_test = split_train_test(df, df['LogBoost'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = sklearn.dummy.DummyRegressor(strategy='median')\n",
    "dummy.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = dummy.predict(X_train)\n",
    "sklearn.metrics.mean_absolute_error(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dummy.predict(X_test)\n",
    "sklearn.metrics.mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DUMMY_SCORE = sklearn.metrics.mean_absolute_error(y_test, y_pred)\n",
    "model_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTRegressor\n",
    "tpot = TPOTRegressor(\n",
    "    random_state=123,\n",
    "    verbosity=2,\n",
    "    n_jobs=-1,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    template='Transformer-Selector-Regressor',\n",
    "    warm_start=True,\n",
    "    memory='auto',\n",
    "    early_stop=10\n",
    ")\n",
    "tpot.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tpot.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tpot.predict(X_test)\n",
    "test_score_tpot = sklearn.metrics.mean_absolute_error(y_test, y_pred)\n",
    "display(test_score_tpot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results['TPOT'] = test_score_tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import plot_partial_dependence\n",
    "clf = tpot.fitted_pipeline_\n",
    "fig, axes = plt.subplots(11, 1, figsize=(4, 16))\n",
    "plot_partial_dependence(clf, X_train, X_train.columns, ax=axes)\n",
    "plt.tight_layout()\n",
    "# plot_partial_dependence(clf, X_train, [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = tpot.predict(X_test)\n",
    "plt.scatter(yhat, y_test)\n",
    "plt.plot([0, 2], [0, 2], 'k--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = tpot.predict(X_train)\n",
    "plt.scatter(yhat, y_train)\n",
    "plt.plot([0, 2], [0, 2], 'k--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unpooled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED =42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpooled_model_factory(X, y):\n",
    "    coords = {\"obs_id\": np.arange(X.shape[0])}\n",
    "    with pm.Model(coords=coords) as model:\n",
    "        days_to_stemperiode = pm.Data('days_to_stemperiode', X['DaysToStemperiode'], dims='obs_id')\n",
    "        logpopularity = pm.Data('logpopularity', X['LogPopularityNorm'], dims='obs_id')\n",
    "        jaren_geleden = pm.Data(\"jaren_geleden\", X['JarenGeleden'], dims='obs_id')\n",
    "        passing_too_early = pm.Data('passing_too_early', X['PassingTooEarly'], dims='obs_id')\n",
    "        is_dutch = pm.Data('is_dutch', X['IsDutchArtist'], dims='obs_id')\n",
    "\n",
    "        multiple_performers = pm.Data('multiple_performers', X['MultiplePerformers'], dims=\"obs_id\")\n",
    "        popularity_within_oeuvre = pm.Data('popularity_within_oeuvre', X['LogSongPopularityWithinArtist'], dims=\"obs_id\")\n",
    "\n",
    "        # Hyperpriors:\n",
    "        a = pm.Normal(\"a\", mu=0, sigma=2.0)\n",
    "\n",
    "        recency_effect_exponent = pm.Normal('recency_effect_exponent', mu=-1.5,sigma=1)\n",
    "        max_recency_effect = pm.Normal('max_recency_effect', mu=2, sigma=2)\n",
    "        effect_popularity = pm.Normal('effect_popularity', mu=0, sigma=2)\n",
    "        history_effect = pm.Normal('history_effect', mu=0, sigma=1)\n",
    "        age_passing_effect = pm.Normal('age_passing_effect', mu=0, sigma=1)\n",
    "        is_dutch_effect = pm.Normal('is_dutch_effect', mu=0, sigma=2)\n",
    "\n",
    "        # Expected value per artist:\n",
    "        mu_artist = (a\n",
    "                     + logpopularity * effect_popularity\n",
    "                     # The correction of subtracting the minimum value is important for two reasons:\n",
    "                     # 1. Since it fixes the minimum value at 1, it breaks the degeneracy with _a_, which makes sampling much more stable\n",
    "                     # 2. It allows for much easier interpretation\n",
    "                     + (np.exp(10**recency_effect_exponent * days_to_stemperiode)- np.exp(10**recency_effect_exponent * -365))* max_recency_effect\n",
    "                     + jaren_geleden * history_effect\n",
    "                     + passing_too_early * age_passing_effect\n",
    "                     + is_dutch * is_dutch_effect\n",
    "                    )\n",
    "\n",
    "        sharing_effect = pm.Normal('sharing_effect', mu=0, sigma=2.0)\n",
    "        within_oeuvre_effect = pm.Normal('within_oeuvre_effect', mu=0, sigma=2.0)\n",
    "        theta = (mu_artist\n",
    "                 + multiple_performers * sharing_effect\n",
    "                 + popularity_within_oeuvre * within_oeuvre_effect\n",
    "                )\n",
    "        # Model error:\n",
    "        sigma = pm.Exponential(\"sigma\", 1.0)\n",
    "\n",
    "        y_like = pm.Normal(\"y_like\", theta, sigma=sigma, observed=y, dims=\"obs_id\")\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with unpooled_model_factory(X=df_train,\n",
    "                   y=y_train,\n",
    "                   ) as multilevel_noncentered_model:\n",
    "    display(pm.model_to_graphviz(multilevel_noncentered_model))\n",
    "    unpooled_model_idata = pm.sample(10000, tune=3000, return_inferencedata=True, random_seed=RANDOM_SEED, target_accept=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with unpooled_model_factory(X=df_test,\n",
    "                   y=y_test,\n",
    "                   ) as test_model:\n",
    "    ppc_unpooled = pm.fast_sample_posterior_predictive(unpooled_model_idata.posterior,\n",
    "                                         var_names=['y_like'],\n",
    "                                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_unpooled = np.median(ppc_unpooled['y_like'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ppc_unpooled['y_like']).median().sub(y_test.values).abs().agg(['mean', 'sem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results['Unpooled'] = sklearn.metrics.mean_absolute_error(y_test, y_pred_unpooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partially pooled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import arviz as az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "def model_factory(X, y):\n",
    "    passed_away_artists = X['ArtistID'].unique()\n",
    "    artist_lookup = dict(zip(passed_away_artists, range(len(passed_away_artists))))\n",
    "    artist_vals = X['ArtistID'].replace(artist_lookup).values\n",
    "    artist_model = (X.assign(ArtistIDModel = lambda df: X['ArtistID'].map(artist_lookup),\n",
    "                             )\n",
    "                .sort_values('ArtistIDModel')\n",
    "                .drop_duplicates(['ArtistIDModel'])\n",
    "               )\n",
    "    \n",
    "    \n",
    "    coords = {\"obs_id\": np.arange(X.shape[0]),\n",
    "              'Artist': range(len(passed_away_artists))\n",
    "         }\n",
    "    with pm.Model(coords=coords) as model:\n",
    "        artist_idx = pm.Data(\"artist_idx\", artist_vals, dims=\"obs_id\")\n",
    "        days_to_stemperiode = pm.Data('days_to_stemperiode', artist_model['DaysToStemperiode'], dims='Artist')\n",
    "        logpopularity = pm.Data('logpopularity', artist_model['LogPopularityNorm'], dims='Artist')\n",
    "        jaren_geleden = pm.Data(\"jaren_geleden\", artist_model['JarenGeleden'], dims='Artist')\n",
    "        passing_too_early = pm.Data('passing_too_early', artist_model['PassingTooEarly'], dims='Artist')\n",
    "        is_dutch = pm.Data('is_dutch', artist_model['IsDutchArtist'], dims='Artist')\n",
    "\n",
    "        multiple_performers = pm.Data('multiple_performers', X['MultiplePerformers'], dims=\"obs_id\")\n",
    "        popularity_within_oeuvre = pm.Data('popularity_within_oeuvre', X['LogSongPopularityWithinArtist'], dims=\"obs_id\")\n",
    "\n",
    "        # Hyperpriors:\n",
    "        a = pm.Normal(\"a\", mu=0, sigma=2.0)\n",
    "        sigma_a = pm.Exponential(\"sigma_a\", 1.0)\n",
    "\n",
    "        recency_effect_exponent = pm.Normal('recency_effect_exponent', mu=-1.5,sigma=1)\n",
    "        max_recency_effect = pm.Normal('max_recency_effect', mu=2, sigma=2)\n",
    "        effect_popularity = pm.Normal('effect_popularity', mu=0, sigma=2)\n",
    "        history_effect = pm.Normal('history_effect', mu=0, sigma=1)\n",
    "        age_passing_effect = pm.Normal('age_passing_effect', mu=0, sigma=1)\n",
    "        is_dutch_effect = pm.Normal('is_dutch_effect', mu=0, sigma=2)\n",
    "\n",
    "        # Expected value per artist:\n",
    "        mu_artist = (a\n",
    "                     + logpopularity * effect_popularity\n",
    "                     # The correction of subtracting the minimum value is important for two reasons:\n",
    "                     # 1. Since it fixes the minimum value at 1, it breaks the degeneracy with _a_, which makes sampling much more stable\n",
    "                     # 2. It allows for much easier interpretation\n",
    "                     + (np.exp(10**recency_effect_exponent * days_to_stemperiode)- np.exp(10**recency_effect_exponent * -365))* max_recency_effect\n",
    "                     + jaren_geleden * history_effect\n",
    "                     + passing_too_early * age_passing_effect\n",
    "                     + is_dutch * is_dutch_effect\n",
    "                    )\n",
    "\n",
    "        # This is the non-centered version of the model for a much more stable sampling\n",
    "        # See https://twiecki.io/blog/2017/02/08/bayesian-hierchical-non-centered/ for more information\n",
    "        mu_artist = pm.Deterministic(\"mu_artist\", mu_artist, dims=\"Artist\")\n",
    "        za_artist = pm.Normal(\"za_artist\", mu=0.0, sigma=1.0, dims='Artist')\n",
    "        a_artist = pm.Deterministic(\"a_artist\", mu_artist + za_artist * sigma_a, dims=\"Artist\")\n",
    "        sharing_effect = pm.Normal('sharing_effect', mu=0, sigma=2.0)\n",
    "        within_oeuvre_effect = pm.Normal('within_oeuvre_effect', mu=0, sigma=2.0)\n",
    "        theta = (a_artist[artist_idx]\n",
    "                 + multiple_performers * sharing_effect\n",
    "                 + popularity_within_oeuvre * within_oeuvre_effect\n",
    "                )\n",
    "        # Model error:\n",
    "        sigma = pm.Exponential(\"sigma\", 1.0)\n",
    "\n",
    "        y_like = pm.Normal(\"y_like\", theta, sigma=sigma, observed=y, dims=\"obs_id\")\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_factory(X=df_train,\n",
    "                   y=y_train,\n",
    "                   ) as multilevel_noncentered_model:\n",
    "    display(pm.model_to_graphviz(multilevel_noncentered_model))\n",
    "    multilevel_noncentered_model_idata = pm.sample(10000, tune=3000, return_inferencedata=True, random_seed=RANDOM_SEED, target_accept=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_factory(X=df_test,\n",
    "                   y=y_test,\n",
    "                   ) as test_model:\n",
    "    # For newly passed artists, we do not know what za_artist should be\n",
    "    ppc_multilevel = pm.fast_sample_posterior_predictive(multilevel_noncentered_model_idata.posterior.drop_vars(['mu_artist', 'a_artist', 'za_artist']),\n",
    "                                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.concat(multilevel_noncentered_model_idata, az.from_dict(posterior_predictive=ppc_multilevel), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(ppc_multilevel['y_like'].mean(axis=0) - y_test.values).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ppc_multilevel['y_like']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_multilevel = np.median(ppc_multilevel['y_like'], axis=0)\n",
    "sklearn.metrics.mean_absolute_error(y_test, y_pred_multilevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results['Partially pooled'] = sklearn.metrics.mean_absolute_error(y_test, y_pred_multilevel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pos = range(len(model_results), 0, -1)\n",
    "plt.barh(y_pos, list(model_results.values()))\n",
    "plt.gca().set_yticks(y_pos)\n",
    "plt.gca().set_yticklabels(labels=list(model_results.keys()))\n",
    "plt.gca().axvline(DUMMY_SCORE, c='k', ls='--', label='Random')\n",
    "plt.xlabel('Mean absolute error\\n(lower is better)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compare  = az.compare({'Multilevel': multilevel_noncentered_model_idata,\n",
    "                             'Unpooled': unpooled_model_idata},\n",
    "                           )\n",
    "display(model_compare)\n",
    "az.plot_compare(model_compare, insample_dev=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilevel_noncentered_model_idata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.loo_pit(idata=multilevel_noncentered_model_idata, y=y_train.values, y_hat=ppc_multilevel['y_like'].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = az.plot_posterior(multilevel_noncentered_model_idata.posterior, var_names=['~za_artist', '~mu_artist', '~a_artist', '~sigma_a'], hdi_prob='hide')\n",
    "az.plot_posterior(unpooled_model_idata.posterior, var_names=['~sigma_a'], ax=axes, c='black', hdi_prob='hide')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

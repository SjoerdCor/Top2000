{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model outcomes and functional check\n",
    "Now that we have confidence that the MCMC has worked, it is of course paramount that we check that the outcomes make sense. \n",
    "\n",
    "### Individual parameter estimates\n",
    "Firstly, let's have a quick look at the summary. We see that all parameters are positive, except for the `recency_effect_exponent` (which is not a size anyway) and the `sharing_effect` (duets are supposed to have a smaller boost), which is as we expect. \n",
    "Furthermore, we see that all parameters are on the same scale - there is no single paramtere dominating all the others - and there are no parameters with huge standard deviations. So far, so good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(multilevel_noncentered_model_idata, var_names=['~za_artist', '~a_artist', '~mu_artist'], round_to=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get more feeling with the parameter values, we compare them to our priors; what we believed to be true beforehand. Generally, we expect the posterior parameters to be tighter that the priors and in the same region. Furthermore, we look at whether the shape of the parameter distributions is approximately correct.\n",
    "\n",
    "Technically, we can again use the InferenceData and add the data. Then `arviz`-package has out-of-the-box functions that compare the prior and the posterior. Sampling from the prior is just as easy: just call `pm.sample_prior_predictive` within a model context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with multilevel_noncentered_model:\n",
    "    prior_checks = pm.sample_prior_predictive(random_seed=RANDOM_SEED)\n",
    "multi_prior = az.from_dict(prior={k: v.T for k, v in prior_checks.items() if k != 'y_like'})\n",
    "multilevel_noncentered_model_idata.extend(multi_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_priors = ['a',\n",
    "               'recency_effect_exponent',\n",
    "               'max_recency_effect',\n",
    "               'effect_popularity',\n",
    "               'history_effect',\n",
    "               'age_passing_effect',\n",
    "               'is_dutch_effect',\n",
    "               'sharing_effect',\n",
    "               'within_oeuvre_effect',\n",
    "               'sigma',\n",
    "               'sigma_a',\n",
    "              ]\n",
    "\n",
    "axes = az.plot_dist_comparison(multilevel_noncentered_model_idata, var_names=vars_priors, figsize=(12, 5*len(vars_priors)))\n",
    "plt.savefig('Prior_vs_posterior.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the posteriors are indeed more constrained than, and in line with the priors. One point of interest is that the history effect seems to be slightly informed by the prior - this is not generally a bad thing, but this prior is not very well substantiated, so we could have considered widening it a bit. Other than that, everything seems to be fine.\n",
    "\n",
    "Next we look at whether the parameters are significantly different from 0. This turns out to be the case for all parameters except for `is_dutch_effect` and `within_oeuvre_effect`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = az.plot_posterior(multilevel_noncentered_model_idata, ref_val=0, var_names=var_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the recency effect\n",
    "\n",
    "A slightly more difficult instance to look at is the recency effect, since it consists of two parameters: a recency effect exponent (determining the shape) and the maximum effect (determining the height). We sample from the posterior to get full lines that estimate the effect, which give us insight into the best estimate of the full effect, as well as the uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = multilevel_noncentered_model_idata.posterior.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outcomes = {}\n",
    "days = np.arange(-365, 0)\n",
    "\n",
    "parameters_without_groups = parameters.droplevel('Artist').loc[lambda x: ~x.index.duplicated()]\n",
    "\n",
    "for i, row in parameters_without_groups.iterrows():\n",
    "    recencyeffect = (np.exp(10**row['recency_effect_exponent'] * days) - np.exp(10**row['recency_effect_exponent'] * -365))* row['max_recency_effect']\n",
    "    outcomes[i] = recencyeffect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recencyeffect = pd.DataFrame(outcomes, index=np.arange(-365, 0)).apply(np.exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_lines = 1500\n",
    "fig, ax = plt.subplots()\n",
    "df_recencyeffect.mean(axis='columns').plot(ax=ax, lw=3, c='darkblue', label='Estimated effect')\n",
    "(df_artist.assign(RecencyEffect = lambda df: df['Boost']/df['Boost'].median())\n",
    "          .query('DaysToStemperiode > -400')\n",
    "          .plot(x='DaysToStemperiode', y='RecencyEffect', kind='scatter', ax=ax, c='k', alpha=0.2, label='Passed away artists')\n",
    ")\n",
    "plt.legend()\n",
    "df_recencyeffect.sample(N_lines, axis='columns').plot(c='grey', alpha=0.01, legend=False, ax=ax)\n",
    "\n",
    "ax.set_xlabel('Days until end of voting')\n",
    "ax.set_ylabel('Extra boost')\n",
    "plt.savefig('Boost_vs_recency.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_date(days_from_date, reference_date='2020-12-01'):\n",
    "    return pd.Timestamp(reference_date) + pd.Series([pd.Timedelta(i, 'days') for i in days_from_date], index=days_from_date)\n",
    "\n",
    "ax =  (df_recencyeffect.mean(axis='columns').to_frame()\n",
    "     .assign(Date = lambda df: convert_to_date(df.index))\n",
    "     .plot(x='Date', y=0, lw=3, c='darkblue', label='Estimated effect')\n",
    "    )\n",
    "\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "plt.ylabel('Extra boost')\n",
    "plt.xlabel('Date of passing')\n",
    "plt.yticks([1, 1.5, 2, 2.5])\n",
    "plt.savefig(os.path.join(FOLDER_OUTPUT, 'recency_effect.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_to_use = [-365,\n",
    "               *range(-350, -50, 50),\n",
    "               *range(-70, -10, 7),\n",
    "               *range(-10, 0, 2),\n",
    "              ]\n",
    "df_recencyeffect.quantile([0.025, 0.16, 0.5, 0.84, 0.975], axis='columns').loc[:, days_to_use]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint distributions\n",
    "After looking at all parameters indivudally, I look at the parameters in joint distribution, which reveals linear and non-linear correlations. We see mostly very regular joint distributions, but the recency effects have a very interesting 'whale-like' distribution: a mostly positive correlation, which turns into a negative correlation for very small exponents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.rcParams['plot.max_subplots'] = 100  # Since we have many parameters, the number of subplots is larger than the default - allow az to take more time plotting\n",
    "_ = az.plot_pair(multilevel_noncentered_model_idata,\n",
    "                var_names=var_names, marginals=True,\n",
    "                divergences=True,\n",
    "                kind=['scatter', 'kde'],\n",
    "                figsize=(30, 30),\n",
    "                scatter_kwargs={'alpha': 0.06})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlations(df):\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    ax = sns.heatmap(df.corr(), cmap='RdBu_r', vmin=-0.8, vmax=0.8, annot=True, fmt='.1%', ax=ax, cbar=False)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quantify this a bit, lets look into the (linear) correlations of the parameters of the effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlations(parameters_without_groups.drop(columns=['za_artist', 'mu_artist', 'a_artist']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a number of interesting things:\n",
    "* There is a strong degeneracy between $a$ and $\\textrm{history_effect}$, which means that the model finds it easier to determing the boost further out in the past\n",
    "* Interestingly, the correlation coefficient is quite small between $\\textrm{max_recency_effect}$ and $\\textrm{recency_effect_exponent}$. While they are strongly correlated, they are not _linearly_ correlated.\n",
    "\n",
    "#### The importance of artist deviation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit/prediction quality\n",
    "\n",
    "Now that we have a good understanding of the fit, working and parameters of our model, it is finally time to look at the predictions. To do this  we sample from the posterior predictive, which means we use every sample from our MCMC chain, and calculate what the predicted value is for boost. We do in this in two ways:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with multilevel_noncentered_model:\n",
    "    ppc = pm.fast_sample_posterior_predictive(multilevel_noncentered_model_idata)\n",
    "    # fast_sample throws a mysterious error when dropping some vars\n",
    "    ppc_no_artist = pm.sample_posterior_predictive(multilevel_noncentered_model_idata.posterior.drop_vars(['mu_artist', 'a_artist', 'za_artist']))\n",
    "    \n",
    "predictions = pd.DataFrame(ppc['y_like'].T, index=df.index)\n",
    "predictions_no_artist = pd.DataFrame(ppc_no_artist['y_like'].T, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_agg = (pd.concat([predictions.quantile([0.16, 0.5, 0.84], axis='columns').transpose(),\n",
    "                              df['LogBoost']], axis='columns')\n",
    "                   .rename(columns={0.5: 'yhat'})\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = predictions_agg.plot(x='yhat', y='LogBoost', kind='scatter')\n",
    "valmin, valmax = predictions_agg[['LogBoost', 'yhat']].min().min(), predictions_agg[['LogBoost', 'yhat']].max().max()\n",
    "ax.plot([valmin, valmax], [valmin, valmax], 'k--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the predicted boost matches the actual boost very well. For easier interpretation, let's transform back to the linear space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_exp = predictions_agg.apply(np.exp).rename(columns={'LogBoost': 'Boost'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(predictions_exp['yhat'], predictions_exp['Boost'],\n",
    "             xerr=[predictions_exp['yhat'].sub(predictions_exp[0.16]), predictions_exp[0.84].sub(predictions_exp['yhat'])],\n",
    "             ls=' ', marker='o', alpha=0.6, ms=4)\n",
    "plt.gca().annotate('$\\it{Zij\\/gelooft\\/in\\/mij}$\\n by $\\it{André\\/Hazes}$', (2.123288,17.81695), (2.5, 14),\n",
    "                   arrowprops=dict(arrowstyle=\"->\", connectionstyle= \"angle3,angleA=0,angleB=90\"),\n",
    ")\n",
    "plt.plot([0, 8], [0, 8], 'k--')\n",
    "plt.ylabel('Boost in practice')\n",
    "plt.xlabel('Predicted boost')\n",
    "plt.savefig(os.path.join(FOLDER_OUTPUT, 'Allboosts_compared.jpg'))\n",
    "plt.show()\n",
    "plt.errorbar(predictions_exp['yhat'], predictions_exp['Boost'],\n",
    "             xerr=[predictions_exp['yhat'].sub(predictions_exp[0.16]), predictions_exp[0.84].sub(predictions_exp['yhat'])],\n",
    "             ls=' ', marker='o', alpha=0.2, ms=4)\n",
    "plt.plot([0, 11], [0, 11], 'k--')\n",
    "plt.ylabel('Boost in practice')\n",
    "plt.xlabel('Predicted boost')\n",
    "plt.ylim(0, 11.5)\n",
    "plt.xlim(0, 11.5)\n",
    "plt.savefig(os.path.join(FOLDER_OUTPUT, 'boostcompared.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that almost all predictions are very well, but _Zij gelooft in mij_ is very off, which may require further investigation in the future, such as investigating extra effects that explain the huge boost (e.g. a $\\textrm{chanelling}$ effect where the boost is channeled through a single song instead of through all songs) or a different model such as a Students-T-distribution instead of a Normal distribution.\n",
    "\n",
    "To get a better picture of the performance of our model, we compare three types of predictions:\n",
    "* The full model, which gives us insight in the quality of the model\n",
    "* The model without the artist specific measures, which can only be estimate _after the death_ of an artist; this will give us more insight in how well we can expect to predict out of sample\n",
    "* The model based solely on the mean parameters, which loses a lot of information about the uncertainty, but is easier to reason about #TODO explain mean = expectation value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = top2000analysis.BoostExplainer(parameters.mean(), multilevel_noncentered_model_idata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = (df\n",
    "              .assign(yhat = predictions.mean(axis='columns'),\n",
    "                      error = lambda df: df['yhat'].sub(df['LogBoost']),\n",
    "                      abserror = lambda df: df['error'].abs(),\n",
    "                      yhat_no_artist = predictions_no_artist.mean(axis='columns'),\n",
    "                      error_without_artisteffect = lambda df: df['yhat_no_artist'].sub(df['LogBoost']),\n",
    "                      abserror_without_artisteffect = lambda df: df['error_without_artisteffect'].abs(),\n",
    "                      improvement_artisteffect = lambda df: df['abserror_without_artisteffect'].sub(df['abserror']),\n",
    "                      yhat_mean_params = lambda df: np.log([b.all_effects(i)['EffectSize'].prod() for i in range(len(df))]),\n",
    "                      error_mean_params = lambda df: df['yhat_mean_params'].sub(df['LogBoost']),\n",
    "                      abserror_mean_params = lambda df: df['error_mean_params'].abs(),\n",
    "                     )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior.nlargest(10, 'abserror').filter(['Year', 'Rank', 'Title', 'NameArtist', 'Boost', 'LogBoost', 'yhat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior[['LogBoost',\n",
    "           'yhat',\n",
    "           'yhat_no_artist',\n",
    "           'yhat_mean_params']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the predictions of the full model and the true values correlate very well ($\\rho > 0.8$), and that while the full model correlates very well with the model without the artist information, the latter indeed correlates worse with the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_error = posterior['LogBoost'].sub(posterior['LogBoost'].median()).abs().mean()\n",
    "maes = pd.Series(\n",
    "        {\n",
    "            'No artist model': posterior['abserror_without_artisteffect'].mean(),\n",
    "            'Full model': posterior['abserror'].mean(),\n",
    "        })\n",
    "ax = maes.plot(kind='barh')\n",
    "ax.axvline(dummy_error, c='k', ls='--', label='Dummy model:\\nPredicting the median')\n",
    "ax.set_xlabel('Mean absolute error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that both models are much better that a dummy model that predicts the median always. We do see however that we lose quite a bit of information that is in the artist specific model. There is not much we can do about that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information in the artist specific effect\n",
    "\n",
    "We see that the artist effect on average improves the prediction by ~10%, but there are also cases where it worsens the prediction, and where it improves the predicting by more than 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(posterior['improvement_artisteffect'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the distribution for the artist effect strength is standard normally distributed. On further inspection, we see that Michael Jackson received the strongest boost - which makes sense considering the soap around his death, and Joe Cocker and Liesbeth List got smaller boosts than expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes, artistnames = pd.factorize(df['NameArtist'])\n",
    "mapping = {c: a for c, a in zip(set(codes), artistnames)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_magic = multilevel_noncentered_model_idata.posterior.to_dataframe().groupby('Artist')['za_artist'].describe()\n",
    "artist_magic.index = artist_magic.index.map(mapping)\n",
    "artist_magic.sort_values('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In depth explanation of predictions\n",
    "\n",
    "While we have looked at the model globally, to get a more concrete view we also look into specific cases. To do this, we select 'representative samples', so we do not have to look into 153 songs.\n",
    "\n",
    "## Finding representative samples\n",
    "First, we select representative samples. There are many ways to do this, but since we are using a linear-regression-like model it works best if the input data is as different for the two samples as possible.\n",
    "\n",
    "To do this, we compute the distances between input parameter of each of the 153 songs of passed away artists. For the recency effect, we first transform it to account for the fact that the relation is exponential. Also we transform all variables to a unit scale, so every variable weighs evenly.\n",
    "\n",
    "Then we select all songs that have an error that smaller than or equal to the mean absolute error (approximately), because then we will understand why the model chose the prediction, and we select songs by artists that are at least moderately popular, so they will be known to the public.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['JarenGeleden',\n",
    "             'LogPopularityNorm',\n",
    "             'IsDutchArtist',\n",
    "             'PassingTooEarly',\n",
    "             'DaysEffect',\n",
    "             'LogSongPopularityWithinArtist',\n",
    "             'MultiplePerformers',          \n",
    "             'LogBoost']\n",
    "mm = sklearn.preprocessing.MinMaxScaler()\n",
    "\n",
    "all_data = (posterior\n",
    "            .assign(DaysEffect = lambda df: df_recencyeffect.mean(axis='columns').loc[df['DaysToStemperiode'].clip(lower=-365)].tolist())\n",
    "            .filter(variables)\n",
    "            )\n",
    "mm.fit(all_data)\n",
    "\n",
    "data = (posterior\n",
    "        .query('abserror < 0.15 & LogPopularityNorm > -0.3 & DaysToStemperiode < -2')\n",
    "        .query('Title != \"Never Be Clever\"') # Exclude this song because\n",
    "        .assign(DaysEffect = lambda df: df_recencyeffect.mean(axis='columns').loc[df['DaysToStemperiode'].clip(lower=-365)].tolist())\n",
    "        .filter(variables)\n",
    "       )\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = mm.transform(data)\n",
    "dists = sklearn.metrics.pairwise_distances(normalized_data, metric='minkowski', p=1)\n",
    "inds = np.argsort(dists, axis=None)[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at the Top 5 of song pairs which are as different as popular. We exclude the first pair of Montserrat Caballé and André Hazes, because Caballé was only noted with a duet with Freddy Mercury, which is already a bit off, but mostly because she passed away in the year the movie _Bohemian Rhapsody_ was released, which may have caused a boost for everything that's Queen-related. Therefore, we select the second-most different pair of David Bowie's _Under Pressure_ (a duet, Bowie passed away only 4 years ago) and André Hazes (passed away longer ago, is Dutch, passed away relatively shortly before the Top 2000 voting period and while he was younger). Unfortunately, the difference in popularity is quite small, but that is a compromise we must be willing to accept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boosting_effects = ['LogBoost', 'yhat', 'error','yhat_no_artist', 'error_without_artisteffect']\n",
    "variables = [v for v in variables if v != 'DaysEffect']\n",
    "# Use step size of 2 because of symmetry: each pair is present twice\n",
    "for ind in inds[:10:2]:\n",
    "    x, y = divmod(ind, len(data))\n",
    "    print(f'Distance: {dists[x, y]:.3f}; ({x}, {y})')\n",
    "    ind = data.iloc[[x, y]].index\n",
    "\n",
    "    new_df = pd.concat([df.assign(pos = range(len(df)))[['NameSong', 'Title', 'LogBoost', 'BoostSong', 'DaysToStemperiode', 'pos'] + variables],\n",
    "                        posterior[boosting_effects]], axis='columns')\n",
    "    display(new_df.loc[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figuring out the boost for two songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can for these two songs calculate how the boost came about; since we are using a linear model on the log boost, we can multiply the effects in the linear space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, include_difference):\n",
    "    df = df.assign(TotalEffect = lambda df: df['EffectSize'].cumprod(),\n",
    "                   Diff = lambda df: df['TotalEffect'].diff().fillna(df['TotalEffect']))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_colours(data, mask_from, including_difference):\n",
    "    if mask_from is None:\n",
    "        if including_difference:\n",
    "            c = ['gray'] + (len(data) - 4) * ['lightgray'] + ['purple', 'red', 'purple']\n",
    "            return c\n",
    "        else:\n",
    "            mask_from = len(data)\n",
    "    if mask_from == 1:\n",
    "        c =  ['purple']\n",
    "    elif mask_from <= len(data):\n",
    "        c = ['gray'] + (mask_from - 2) * ['lightgray'] + ['purple']\n",
    "    return c\n",
    "    \n",
    "def plot_waterfall(effects, mask_from, including_difference=False, ax=None, horizontal=False):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    data = effects.copy()\n",
    "    #Store data and create a blank series to use for the waterfall\n",
    "    total = data['Diff'].sum()\n",
    "\n",
    "    if mask_from is not None:\n",
    "        data.iloc[mask_from:] = None\n",
    "    blank = data['Diff'].cumsum().shift(1).fillna(0)\n",
    "\n",
    "    #Get the net total number for the final element in the waterfall\n",
    "    if mask_from is None:\n",
    "        data.loc[(\"Total\", ''), 'EffectSize'] = 1\n",
    "        data.loc[(\"Total\", ''), 'Diff'] = total\n",
    "        data.loc[(\"Total\", ''), 'TotalEffect'] = total\n",
    "        blank.loc[\"Total\"] = total # This is only to get the steps right - it will later correctly be set to 0\n",
    "\n",
    "    #The steps graphically show the levels as well as used for label placement\n",
    "    step = blank.reset_index(drop=True).repeat(3).shift(-1)\n",
    "    step[1::3] = np.nan\n",
    "    if mask_from is None:\n",
    "        blank.loc[\"Total\"] = 0\n",
    "    else:\n",
    "        step.iloc[mask_from * 3:] = None\n",
    "    \n",
    "    if including_difference:\n",
    "        blank.loc[('Prediction', '')] = 0\n",
    "        data.loc[('Prediction', ''), 'Diff'] = data.loc[('Prediction', ''), 'TotalEffect']\n",
    "\n",
    "    #Plot and label\n",
    "    colours = find_colours(data, mask_from, including_difference)\n",
    "    kind = 'bar' if not horizontal else 'barh'\n",
    "    ax = data['Diff'].plot(kind=kind,\n",
    "                           stacked=True,\n",
    "                           bottom=blank,\n",
    "                           left=blank,\n",
    "                           legend=None,\n",
    "                           color=colours,\n",
    "                           ax=ax)\n",
    "    if horizontal:\n",
    "        ax.plot(step.values, step.index, 'k', linewidth=1)\n",
    "    else:\n",
    "        ax.plot(step.index, step.values, 'k', linewidth=1)\n",
    "\n",
    "    #Get the y-axis position for the labels\n",
    "    y_height = data['Diff'].cumsum().shift(1).fillna(0)\n",
    "\n",
    "    #Get an offset so labels don't sit right on top of the bar\n",
    "    vmax = data['Diff'].max()\n",
    "    neg_offset = vmax / 25\n",
    "    pos_offset = vmax / 50\n",
    "    \n",
    "    #Start label loop\n",
    "    loop = 0\n",
    "    for index, row in data.iterrows():\n",
    "        # For the last item in the list, we don't want to double count\n",
    "        y = row['TotalEffect']\n",
    "        # Determine if we want a neg or pos offset\n",
    "        if row['Diff'] >= 0:\n",
    "            y += pos_offset\n",
    "            va = 'bottom'\n",
    "            ha = 'left'\n",
    "        else:\n",
    "            y -= neg_offset\n",
    "            va = 'top'\n",
    "            ha = 'right'\n",
    "        if index not in [('Prediction', ''), ('Total', '')]:\n",
    "            label = f'x {row[\"EffectSize\"]:.2f}'\n",
    "        else:\n",
    "            label = ''\n",
    "        if loop > 0:\n",
    "            label += f'\\n= {row[\"TotalEffect\"] : .2f}'\n",
    "        if horizontal:\n",
    "            ax.annotate(label, (y, loop), va=\"center\", ha=ha, fontsize=11)\n",
    "        else:\n",
    "            ax.annotate(label, (loop, y), ha=\"center\", va=va, fontsize=11)\n",
    "        loop += 1\n",
    "\n",
    "    #Scale up the axis so there is room for the labels\n",
    "    if horizontal:\n",
    "        ax.axvline(1, c='k', ls='--')\n",
    "        ax.set_xlim(0, 3)\n",
    "    else:\n",
    "        ax.axhline(1, c='k', ls='--')\n",
    "        ax.set_ylim(0, 3)\n",
    "    \n",
    "    labels = ['Base',\n",
    "              'Historical\\neffect',\n",
    "              'Popularity\\neffect',\n",
    "              'Artist is\\nDutch',\n",
    "              'Artist\\ndied young',\n",
    "              'Timing of\\ndeath',\n",
    "              'Artist\\ndeviation',\n",
    "              'Song popularity\\nwithin artist oeuvre',\n",
    "              'Multiple\\nperformers',\n",
    "              'Prediction'\n",
    "              ]\n",
    "    if including_difference:\n",
    "        labels += ['Difference from\\nactual boost', 'Actual boost']\n",
    "    if horizontal:\n",
    "        ax.set_yticklabels(labels)\n",
    "        ax.invert_yaxis()\n",
    "    else:\n",
    "        ax.set_xticklabels(labels)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    if horizontal:\n",
    "        ax.set_xlabel('Boost', fontsize=14)\n",
    "        ax.set_ylabel('Effects', fontsize=14)\n",
    "    else:\n",
    "        ax.set_ylabel('Boost', fontsize=14)\n",
    "        ax.set_xlabel('Effects', fontsize=14)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus = None\n",
    "song_pos = 76\n",
    "explanation = b.all_effects(song_pos)\n",
    "data_eenzamekerst = explanation.pipe(preprocess, False)\n",
    "data_eenzamekerst_incl_diff = b.all_effects(song_pos, True, multilevel_noncentered_model_idata).pipe(preprocess, True)\n",
    "\n",
    "song_pos = 1\n",
    "explanation = b.all_effects(song_pos)\n",
    "data_underpressure = explanation.pipe(preprocess, False)\n",
    "data_underpressure_incl_diff = b.all_effects(song_pos, True, multilevel_noncentered_model_idata).pipe(preprocess, True)\n",
    "\n",
    "fig, subplots = plt.subplots(1, 2, figsize=(11, 7))\n",
    "ax = plot_waterfall(data_eenzamekerst_incl_diff, focus, True, subplots[0], True)\n",
    "ax.set_title('Eenzame Kerst by André Hazes')\n",
    "plt.setp(ax.get_xticklabels()[-1], visible=False)\n",
    "\n",
    "ax2 = plot_waterfall(data_underpressure_incl_diff, focus, True, subplots[1], True)\n",
    "ax2.set_title('Under Pressure by David Bowie & Queen')\n",
    "ax2.set_yticklabels([])\n",
    "ax2.set_ylabel('')\n",
    "plt.setp(ax2.get_xticklabels()[0], visible=False)\n",
    "plt.subplots_adjust(wspace=1e-2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'EffectsEKUP_incl_diff_horizontal.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a full description of this plot, see the [blog](https://sjoerdcornelissen.com/2021/03/02/the-effect-of-passing-away-on-top-2000-ranking/). Finally, we make this plot where we mask the final bars to make it easier to look at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 7))\n",
    "plot_waterfall(data_eenzamekerst, 1, False, horizontal=True, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'Effects_base_horizontal.jpg')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for focus in list(range(2, len(data_eenzamekerst) + 1)) + [None]:\n",
    "    fig, subplots = plt.subplots(1, 2, figsize=(11, 7))\n",
    "    \n",
    "    ax = plot_waterfall(data_eenzamekerst, focus, False, subplots[0], True)\n",
    "    ax.set_title('Eenzame Kerst by André Hazes')\n",
    "    plt.setp(ax.get_xticklabels()[-1], visible=False)\n",
    "\n",
    "    ax2 = plot_waterfall(data_underpressure, focus, False, subplots[1], True)\n",
    "    ax2.set_title('Under Pressure by David Bowie & Queen')\n",
    "    ax2.set_yticklabels([])\n",
    "    ax2.set_ylabel('')\n",
    "    plt.setp(ax2.get_xticklabels()[0], visible=False)\n",
    "    \n",
    "    plt.subplots_adjust(wspace=1e-2)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'EffectsEKUP_focus_{focus}_horizontal.jpg')\n",
    "    plt.close(fig) # Do not show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus = None\n",
    "song_pos = 49\n",
    "explanation = b.all_effects(song_pos, True, multilevel_noncentered_model_idata).pipe(preprocess, True)\n",
    "data_carelesswhisper = explanation.pipe(preprocess, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.assign(pos = range(len(df))).query('NameSong == \"George Michael\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 2, figsize=(30, 24))\n",
    "\n",
    "for ax, pos in zip(axes.flatten(), range(49, 57)): \n",
    "    focus = None\n",
    "    song_pos = pos\n",
    "    explanation = b.all_effects(song_pos, True, multilevel_noncentered_model_idata).pipe(preprocess, True)\n",
    "    data_carelesswhisper = explanation.pipe(preprocess, False)\n",
    "    plot_waterfall(data_carelesswhisper, None, horizontal=True, including_difference=True, ax=ax)\n",
    "    ax.set_title(df.iloc[song_pos].loc['Title'])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting unseen instances\n",
    "While the main goal of this exercise was _inference_ (estimate what affects the size of the boost), I will quickly show how to predict theoretical new instances. This is specifically interesting because some parameters are fundanmentally unknowable beforehand, because they belong to new groups (i.e. new artists passing away): the artist specific boost is always known only after the oublishing of the official Top 2000.\n",
    "\n",
    "This turns out to be relatively straight forward, since we made a model factory earlier:\n",
    "1. Give the new data as arguments to the model factory\n",
    "1. Importantly, we must drop the group-level parameters, which relate to specific artist, since they are unknown\n",
    "1. Sample from the posterior predictive, which will generate one prediction for every sample from the Markov Chain Monte Carlo; the mean and its uncertainty give the predicted boost for each song\n",
    "\n",
    "Somewhat surprisingly, PyMC3 requires two samples for every coordinate - and we have a coordinate for every song and every artist, so we require at least two unique artists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# NOTE!! It only works with at least 2 different artists (which is far from perfect... but it is what it is)\n",
    "df_new_data = df.tail(2).copy()\n",
    "# We predict the boost for two songs, as if the artist passes away 100 years in the future\n",
    "df_new_data['JarenGeleden'] = 100 \n",
    "\n",
    "# Second comes the hold out data posterior predictive\n",
    "with model_factory(X=df_new_data,\n",
    "                   y=df_new_data['LogBoost'],\n",
    "                   ) as prediction_model:\n",
    "    # For newly passed artists, we do not know what za_artist should be\n",
    "    # Again, the mysterious error for fast_sample pops up\n",
    "    ppc_new_data = pm.sample_posterior_predictive(multilevel_noncentered_model_idata.posterior.drop_vars(['mu_artist', 'a_artist', 'za_artist']),\n",
    "                                         var_names=['y_like'],\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we can add this to the InferenceData object, so we can easily use it in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.concat(multilevel_noncentered_model_idata, \n",
    "          az.from_pymc3_predictions(ppc_new_data, model=prediction_model), inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Top2000H",
   "language": "python",
   "name": "top2000h"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
